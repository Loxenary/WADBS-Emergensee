{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4fb961",
   "metadata": {},
   "source": [
    "# Hand Sign Model Training\n",
    "This file is used to train machine learning models for hand sign recognition. It uses the MediaPipe Hands model to detect hands and the Tensorflow Object Detection API to train a model for object detection. The model is then saved to a file for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f47de3",
   "metadata": {},
   "source": [
    "## Chores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d2e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore, Back, Style\n",
    "class Logger:\n",
    "    @staticmethod\n",
    "    def log_success(message):\n",
    "        print(Fore.GREEN + f\"[SUCCESS] : {message}\" + Style.RESET_ALL)\n",
    "\n",
    "    @staticmethod\n",
    "    def log_error(message):\n",
    "        print(Fore.RED + f\"[ERROR] : {message}\" + Style.RESET_ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d97bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[SUCCESS] : Import Library Success\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Importing Libraries\n",
    "import os\n",
    "import zipfile\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import kaggle\n",
    "\n",
    "Logger.log_success(\"Import Library Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a964d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 'teukumariefafwan/indonesian-sign-language-bahasa-isyarat-indonesia'...\n",
      "Dataset URL: https://www.kaggle.com/datasets/teukumariefafwan/indonesian-sign-language-bahasa-isyarat-indonesia\n",
      "Download complete. Now extracting 'indonesian-sign-language-bahasa-isyarat-indonesia.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting : 100%|██████████| 2600/2600 [00:24<00:00, 106.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete and zip file removed.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Error: Videos path not found at 'BISINDO_Word_Dataset\\BISINDO Sign Language Dataset\\Video'. Check the folder structure.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset folder \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m already exists.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(VIDEOS_PATH):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: Videos path not found at \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVIDEOS_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Check the folder structure.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Error: Videos path not found at 'BISINDO_Word_Dataset\\BISINDO Sign Language Dataset\\Video'. Check the folder structure."
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import kaggle\n",
    "import os\n",
    "\n",
    "KAGGLE_DATASET = \"teukumariefafwan/indonesian-sign-language-bahasa-isyarat-indonesia\"\n",
    "DATASET_FOLDER = \"BISINDO_Word_Dataset\"\n",
    "VIDEOS_PATH = os.path.join(DATASET_FOLDER, \"BISINDO Sign Language Dataset\", \"Video\")\n",
    "\n",
    "# Define the expected zip file name based on the dataset slug\n",
    "zip_filename = f\"{KAGGLE_DATASET.split('/')[1]}.zip\"\n",
    "\n",
    "if not os.path.exists(DATASET_FOLDER):\n",
    "    print(f\"Downloading dataset '{KAGGLE_DATASET}'...\")\n",
    "    try:\n",
    "        # Step 1: Download the file WITHOUT unzipping it\n",
    "        # After\n",
    "        kaggle.api.dataset_download_files(KAGGLE_DATASET, path='.', unzip=False)\n",
    "        print(f\"Download complete. Now extracting '{zip_filename}'...\")\n",
    "\n",
    "        # Step 2: Unzip the file manually with a tqdm progress bar\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zf:\n",
    "            # Wrap zf.infolist() with tqdm to create the progress bar\n",
    "            for member in tqdm(zf.infolist(), desc='Extracting '):\n",
    "                try:\n",
    "                    # Extract each file into the target folder\n",
    "                    zf.extract(member, path=DATASET_FOLDER)\n",
    "                except zipfile.error as e:\n",
    "                    print(f\"Error extracting file {member.filename}: {e}\")\n",
    "        \n",
    "        # Clean up the downloaded zip file after extraction\n",
    "        os.remove(zip_filename)\n",
    "        print(\"Extraction complete and zip file removed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(f\"Dataset folder '{DATASET_FOLDER}' already exists.\")\n",
    "\n",
    "if not os.path.exists(VIDEOS_PATH):\n",
    "    raise FileNotFoundError(f\"Error: Videos path not found at '{VIDEOS_PATH}'. Check the folder structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 30  # Number of frames per video to analyze\n",
    "NUM_LANDMARKS = 21 * 3 # 21 landmarks * (x, y, z)\n",
    "DATA_FILE = \"landmark_sequences.npy\"\n",
    "\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    print(f\"'{DATA_FILE}' not found. Starting video processing...\")\n",
    "    mp_hands = mp.solutions.hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "    \n",
    "    sequences, labels = [], []\n",
    "    \n",
    "    # The dataset has category folders (e.g., 'WH-Question'), and inside them are the word videos\n",
    "    for category in os.listdir(VIDEOS_PATH):\n",
    "        category_path = os.path.join(VIDEOS_PATH, category)\n",
    "        if not os.path.isdir(category_path): continue\n",
    "\n",
    "        for video_file in tqdm(os.listdir(category_path), desc=f\"Processing {category}\"):\n",
    "            video_path = os.path.join(category_path, video_file)\n",
    "            word_label = os.path.splitext(video_file)[0] # Label is the video filename without extension\n",
    "\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            video_sequence = []\n",
    "\n",
    "            for frame_num in range(SEQUENCE_LENGTH):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break # Break if video ends early\n",
    "                \n",
    "                results = mp_hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                if results.multi_hand_landmarks:\n",
    "                    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in results.multi_hand_landmarks[0].landmark]).flatten()\n",
    "                    video_sequence.append(landmarks)\n",
    "                else:\n",
    "                    # Append zeros if no hand is detected to maintain sequence length\n",
    "                    video_sequence.append(np.zeros(NUM_LANDMARKS))\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            # Only include sequences that have the correct length\n",
    "            if len(video_sequence) == SEQUENCE_LENGTH:\n",
    "                sequences.append(video_sequence)\n",
    "                labels.append(word_label)\n",
    "    \n",
    "    mp_hands.close()\n",
    "    \n",
    "    X = np.array(sequences)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    np.save(DATA_FILE, {'features': X, 'labels': y})\n",
    "    print(f\"Processed data saved to '{DATA_FILE}'. Shape: {X.shape}\")\n",
    "else:\n",
    "    print(f\"'{DATA_FILE}' already exists. Loading data.\")\n",
    "    data = np.load(DATA_FILE, allow_pickle=True).item()\n",
    "    X = data['features']\n",
    "    y = data['labels']\n",
    "    print(f\"Data loaded. Shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3fdf23",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701b212",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25210e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset Info\n",
    "# print(df_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d115fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Nullity Check\n",
    "# print(df_raw.isnull().sum())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92779aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First 5 Rows\n",
    "# print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09309d9",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Class Distribution\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.countplot(y='label', data=df_raw, order=df_raw['label'].value_counts().index, palette='viridis')\n",
    "# plt.title('Class Distribution in the Dataset', fontsize=16)\n",
    "# plt.xlabel('Number of Samples', fontsize=12)\n",
    "# plt.ylabel('Gesture Label', fontsize=12)\n",
    "# plt.tight_layout()\n",
    "# plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample Hand Landmarks\n",
    "# HAND_CONNECTIONS = mp.solutions.hands.HAND_CONNECTIONS\n",
    "\n",
    "# def visualize_landmarks(landmarks_row):\n",
    "#     \"\"\"Plots a 2D representation of the hand landmarks.\"\"\"\n",
    "#     landmarks = landmarks_row.values.reshape(21, 3)\n",
    "#     x = landmarks[:, 0]\n",
    "#     y = landmarks[:, 1]\n",
    "    \n",
    "#     plt.figure(figsize=(5, 5))\n",
    "#     plt.scatter(x, y)\n",
    "#     # Invert y-axis to match image coordinates (origin at top-left)\n",
    "#     plt.gca().invert_yaxis()\n",
    "    \n",
    "#     # Draw connections\n",
    "#     for connection in HAND_CONNECTIONS:\n",
    "#         start_idx = connection[0]\n",
    "#         end_idx = connection[1]\n",
    "#         plt.plot([x[start_idx], x[end_idx]], [y[start_idx], y[end_idx]], 'r-')\n",
    "        \n",
    "#     plt.xlabel('X coordinate')\n",
    "#     plt.ylabel('Y coordinate')\n",
    "#     plt.title(f\"Landmark Visualization for Label: {landmarks_row.name}\")\n",
    "#     plt.axis('equal')\n",
    "#     plt.show()\n",
    "\n",
    "# # Visualize one sample from a few different classes\n",
    "# sample_labels = ['A', 'B', 'C', 'D']\n",
    "# for label in sample_labels:\n",
    "#     sample_row = df_raw[df_raw['label'] == label].iloc[0]\n",
    "#     visualize_landmarks(sample_row.drop('label'))\n",
    "\n",
    "# Logger.log_success(\"Sample hand landmarks visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "Logger.log_success(\"EDA complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a810743",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d611508a",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7835f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Data split complete. Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "print(\"Cell 4: Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d31717",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f59cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(RAW_CSV_FILE)\n",
    "# X = df.drop('label', axis=1).values\n",
    "# y = df['label'].values\n",
    "\n",
    "# # Feature Engineering: Normalize landmarks\n",
    "# X_processed = []\n",
    "# for row in X:\n",
    "#     landmarks = row.reshape(21, 3)\n",
    "#     wrist = landmarks[0]\n",
    "    \n",
    "#     # Translation invariance\n",
    "#     relative_landmarks = landmarks - wrist\n",
    "    \n",
    "#     # Scale invariance\n",
    "#     max_dist = np.max(np.linalg.norm(relative_landmarks, axis=1))\n",
    "#     if max_dist == 0: max_dist = 1 # Avoid division by zero\n",
    "#     normalized_landmarks = relative_landmarks / max_dist\n",
    "    \n",
    "#     X_processed.append(normalized_landmarks.flatten())\n",
    "\n",
    "# X_processed = np.array(X_processed)\n",
    "\n",
    "# # Encode labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# print(f\"Data preprocessed. Feature shape: {X_processed.shape}\")\n",
    "# print(\"Cell 5: Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f560a",
   "metadata": {},
   "source": [
    "## Modeling and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99452af3",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da654bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Input shape: (SEQUENCE_LENGTH, NUM_FEATURES) -> (30, 63)\n",
    "    LSTM(64, return_sequences=True, activation='relu', input_shape=(SEQUENCE_LENGTH, NUM_LANDMARKS)),\n",
    "    Dropout(0.5),\n",
    "    LSTM(128, return_sequences=False, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    # Output layer with one neuron per class\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(\"Cell 5: LSTM model defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001)\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks)\n",
    "print(\"Cell 6: Model training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bf352",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19140adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Model Validation ---\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nModel Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix', fontsize=20)\n",
    "plt.ylabel('Actual Label', fontsize=15)\n",
    "plt.xlabel('Predicted Label', fontsize=15)\n",
    "plt.show()\n",
    "print(\"Cell 7: Validation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3021045b",
   "metadata": {},
   "source": [
    "## Error Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b0a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassified_indices = np.where(y_pred != y_test)[0]\n",
    "# if len(misclassified_indices) > 0:\n",
    "#     print(f\"Found {len(misclassified_indices)} misclassified samples. Analyzing a few...\")\n",
    "    \n",
    "#     # Find the original image paths (this is a bit slow but good for analysis)\n",
    "#     df_test_indices = pd.DataFrame({'original_index': X_test.shape[0]*[0]}).index\n",
    "#     df_test_indices = train_test_split(df.index, test_size=0.2, random_state=42, stratify=y)[1]\n",
    "\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     for i, idx in enumerate(misclassified_indices[:5]): # Show first 5 errors\n",
    "#         original_idx = df_test_indices[idx]\n",
    "#         true_label = label_encoder.inverse_transform([y_test[idx]])[0]\n",
    "#         pred_label = label_encoder.inverse_transform([y_pred[idx]])[0]\n",
    "        \n",
    "#         # Reconstruct image path\n",
    "#         image_path = os.path.join(DATASET_PATH, true_label, f\"{true_label}{original_idx % 100 + 1}.jpg\") # This is an assumption on file naming\n",
    "        \n",
    "#         # We will just display the labels as finding the exact image is complex\n",
    "#         print(f\"Sample {i+1}: True='{true_label}', Predicted='{pred_label}'\")\n",
    "\n",
    "# else:\n",
    "#     print(\"No misclassified samples found on the test set. Excellent model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a7db43",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060013ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = 'sibi_word_model.h5'\n",
    "ENCODER_FILE = 'word_label_encoder.pkl'\n",
    "\n",
    "model.save(MODEL_FILE)\n",
    "with open(ENCODER_FILE, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(f\"Model saved to '{MODEL_FILE}'\")\n",
    "print(f\"Label encoder saved to '{ENCODER_FILE}'\")\n",
    "print(\"Cell 8: Export complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02977e0",
   "metadata": {},
   "source": [
    "## TESTING REAL TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838820ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting Real-Time Prediction ---\")\n",
    "print(\"Run this cell to start the webcam feed. Press 'q' in the window to quit.\")\n",
    "\n",
    "# Load artifacts\n",
    "try:\n",
    "    model = tf.keras.models.load_model(MODEL_FILE)\n",
    "    with open(ENCODER_FILE, 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "except (FileNotFoundError, IOError) as e:\n",
    "    print(f\"Error loading artifacts: {e}. Please run all preceding cells.\")\n",
    "    raise\n",
    "\n",
    "# Initialize MediaPipe and OpenCV\n",
    "live_hands = mp.solutions.hands.Hands(model_complexity=0, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# Variables for real-time prediction\n",
    "sequence = []\n",
    "current_prediction = \"\"\n",
    "prediction_confidence = 0.0\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success: continue\n",
    "\n",
    "    image = cv2.flip(image, 1)\n",
    "    results = live_hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        mp.solutions.drawing_utils.draw_landmarks(image, hand_landmarks, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        # Append landmarks to the sequence\n",
    "        landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks]).flatten()\n",
    "        sequence.append(landmarks)\n",
    "        # Keep the sequence at the correct length\n",
    "        sequence = sequence[-SEQUENCE_LENGTH:]\n",
    "\n",
    "        # Make a prediction once we have a full sequence\n",
    "        if len(sequence) == SEQUENCE_LENGTH:\n",
    "            input_data = np.expand_dims(sequence, axis=0)\n",
    "            prediction = model.predict(input_data, verbose=0)\n",
    "            \n",
    "            predicted_class_index = np.argmax(prediction)\n",
    "            prediction_confidence = prediction[0][predicted_class_index]\n",
    "            \n",
    "            # Only update if confidence is high enough\n",
    "            if prediction_confidence > 0.7:\n",
    "                current_prediction = label_encoder.inverse_transform([predicted_class_index])[0]\n",
    "\n",
    "    # Display Prediction\n",
    "    cv2.rectangle(image, (0, 0), (400, 60), (0, 0, 0), -1)\n",
    "    display_text = f'{current_prediction} ({prediction_confidence:.2f})'\n",
    "    cv2.putText(image, display_text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('SIBI Word Recognition', image)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'): break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "live_hands.close()\n",
    "print(\"Webcam feed stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
